{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialization\n",
    ".env file should contain:\n",
    "```properties\n",
    "GEMINI_KEY=\n",
    "OPENROUTER_KEY=\n",
    "```"
   ],
   "id": "5e65af81339b8650"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:21:10.328875Z",
     "start_time": "2026-01-03T13:21:10.312005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)\n",
    "gemini_api_key = os.getenv(\"GEMINI_KEY\")\n",
    "open_router_api_key = os.getenv(\"OPENROUTER_KEY\")"
   ],
   "id": "ff5a90c5c60b98b1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "fe73d2e5fd666014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:21:10.940271Z",
     "start_time": "2026-01-03T13:21:10.340947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import httpx\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pydantic import BaseModel, Field\n",
    "from autogen_agentchat.base import TaskResult\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, Swarm\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import Image as AGImage, CancellationToken  # We will use Image later\n",
    "from autogen_agentchat.messages import TextMessage, MultiModalMessage\n",
    "from autogen_agentchat.ui import Console"
   ],
   "id": "6e41e570f8e22145",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Defining the model Clients\n",
    "Idea here is to use multiple model clients for different usecases. Different models could be good at different tasks. e.g.\n",
    "- Ollama for local inference\n",
    "- Deepseek for vision tasks\n",
    "- Gemini for reasoning tasks\n",
    "- Claude for coding related tasks\n",
    "- GPT-4 for general purpose tasks"
   ],
   "id": "2a29972ea170c44d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:21:35.877993Z",
     "start_time": "2026-01-03T13:21:35.732671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##################\n",
    "# Ollama Client. #\n",
    "##################\n",
    "ollama_client = OllamaChatCompletionClient(model=\"llama3.1:latest\")\n",
    "\n",
    "##########################################\n",
    "# Deepseek free good for simple usecases #\n",
    "##########################################\n",
    "deepseek_client = OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528:free\",\n",
    "    api_key=open_router_api_key,\n",
    "    model_info={\n",
    "        \"family\": \"deepseek\",\n",
    "        \"vision\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False\n",
    "    },\n",
    "    http_client=httpx.AsyncClient(trust_env=False)\n",
    ")\n",
    "\n",
    "###########################################\n",
    "# Gemini very good for reasoning usecases #\n",
    "###########################################\n",
    "gemini_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=gemini_api_key,\n",
    "    http_client=httpx.AsyncClient(trust_env=False)\n",
    ")\n",
    "\n",
    "########################\n",
    "# Testing model Client.#\n",
    "########################\n",
    "question = \"What is the capital of France in 1 word Do not include any special characters. e.g. (Q) What is the Capital of USA (A) Washington\"\n",
    "answer = \"Paris\"\n",
    "user_content = UserMessage(content=question, source=\"user\")\n",
    "ollama = (await ollama_client.create([user_content])).content[:5]\n",
    "deepseek = (await deepseek_client.create([user_content])).content[:5]\n",
    "gemini = (await gemini_client.create([user_content])).content[:5]\n",
    "print(f\"Ollama: {ollama}, Deepseek: {deepseek}, Gemini: {gemini}\")\n",
    "assert ollama == answer and deepseek == answer and gemini == answer"
   ],
   "id": "12369a547c911f30",
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectionError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     35\u001B[39m answer = \u001B[33m\"\u001B[39m\u001B[33mParis\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     36\u001B[39m user_content = UserMessage(content=question, source=\u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m ollama = (\u001B[38;5;28;01mawait\u001B[39;00m ollama_client.create([user_content])).content[:\u001B[32m5\u001B[39m]\n\u001B[32m     38\u001B[39m deepseek = (\u001B[38;5;28;01mawait\u001B[39;00m deepseek_client.create([user_content])).content[:\u001B[32m5\u001B[39m]\n\u001B[32m     39\u001B[39m gemini = (\u001B[38;5;28;01mawait\u001B[39;00m gemini_client.create([user_content])).content[:\u001B[32m5\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/personal/stunning-spork/.venv/lib/python3.12/site-packages/autogen_ext/models/ollama/_ollama_client.py:646\u001B[39m, in \u001B[36mBaseOllamaChatCompletionClient.create\u001B[39m\u001B[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001B[39m\n\u001B[32m    644\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cancellation_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    645\u001B[39m     cancellation_token.link_future(future)\n\u001B[32m--> \u001B[39m\u001B[32m646\u001B[39m result: ChatResponse = \u001B[38;5;28;01mawait\u001B[39;00m future\n\u001B[32m    648\u001B[39m usage = RequestUsage(\n\u001B[32m    649\u001B[39m     \u001B[38;5;66;03m# TODO backup token counting\u001B[39;00m\n\u001B[32m    650\u001B[39m     prompt_tokens=result.prompt_eval_count \u001B[38;5;28;01mif\u001B[39;00m result.prompt_eval_count \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m,\n\u001B[32m    651\u001B[39m     completion_tokens=(result.eval_count \u001B[38;5;28;01mif\u001B[39;00m result.eval_count \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m),\n\u001B[32m    652\u001B[39m )\n\u001B[32m    654\u001B[39m logger.info(\n\u001B[32m    655\u001B[39m     LLMCallEvent(\n\u001B[32m    656\u001B[39m         messages=[m.model_dump() \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m create_params.messages],\n\u001B[32m   (...)\u001B[39m\u001B[32m    660\u001B[39m     )\n\u001B[32m    661\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/personal/stunning-spork/.venv/lib/python3.12/site-packages/ollama/_client.py:983\u001B[39m, in \u001B[36mAsyncClient.chat\u001B[39m\u001B[34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001B[39m\n\u001B[32m    935\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchat\u001B[39m(\n\u001B[32m    936\u001B[39m   \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    937\u001B[39m   model: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    947\u001B[39m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    948\u001B[39m ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001B[32m    949\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    950\u001B[39m \u001B[33;03m  Create a chat response using the requested model.\u001B[39;00m\n\u001B[32m    951\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    980\u001B[39m \u001B[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001B[39;00m\n\u001B[32m    981\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m983\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._request(\n\u001B[32m    984\u001B[39m     ChatResponse,\n\u001B[32m    985\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mPOST\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    986\u001B[39m     \u001B[33m'\u001B[39m\u001B[33m/api/chat\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    987\u001B[39m     json=ChatRequest(\n\u001B[32m    988\u001B[39m       model=model,\n\u001B[32m    989\u001B[39m       messages=\u001B[38;5;28mlist\u001B[39m(_copy_messages(messages)),\n\u001B[32m    990\u001B[39m       tools=\u001B[38;5;28mlist\u001B[39m(_copy_tools(tools)),\n\u001B[32m    991\u001B[39m       stream=stream,\n\u001B[32m    992\u001B[39m       think=think,\n\u001B[32m    993\u001B[39m       logprobs=logprobs,\n\u001B[32m    994\u001B[39m       top_logprobs=top_logprobs,\n\u001B[32m    995\u001B[39m       \u001B[38;5;28mformat\u001B[39m=\u001B[38;5;28mformat\u001B[39m,\n\u001B[32m    996\u001B[39m       options=options,\n\u001B[32m    997\u001B[39m       keep_alive=keep_alive,\n\u001B[32m    998\u001B[39m     ).model_dump(exclude_none=\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[32m    999\u001B[39m     stream=stream,\n\u001B[32m   1000\u001B[39m   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/personal/stunning-spork/.venv/lib/python3.12/site-packages/ollama/_client.py:767\u001B[39m, in \u001B[36mAsyncClient._request\u001B[39m\u001B[34m(self, cls, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    763\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**part)\n\u001B[32m    765\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m--> \u001B[39m\u001B[32m767\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**(\u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._request_raw(*args, **kwargs)).json())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/personal/stunning-spork/.venv/lib/python3.12/site-packages/ollama/_client.py:713\u001B[39m, in \u001B[36mAsyncClient._request_raw\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    711\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    712\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.ConnectError:\n\u001B[32m--> \u001B[39m\u001B[32m713\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(CONNECTION_ERROR_MESSAGE) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mConnectionError\u001B[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Assistant Agent.",
   "id": "2e242c8d9db4e2aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#################\n",
    "# Basic Example #\n",
    "#################\n",
    "scientist_agent = AssistantAgent(name=\"RocketScientist\", model_client=gemini_client)\n",
    "result = await scientist_agent.run(task=\"Explain the theory of relativity in 1 sentence.\")\n",
    "print(f\"{result.messages[-1].content[:500]}\\n{'-'*80}\")\n",
    "\n",
    "###############################################\n",
    "# Example with system message and description #\n",
    "###############################################\n",
    "customer_service_agent = AssistantAgent(\n",
    "    name=\"CustomerServiceAgent\",\n",
    "    description=\"A very very angry and super rude customer service agent.\", # for Humans only.\n",
    "    system_message=\"You are very rude and super angry customer service agent expected to help with customer queries, about products, refunds and shipping\", # for the LLM (controls agent behavior and responses)\n",
    "    model_client=gemini_client)\n",
    "result = await customer_service_agent.run(task=\"Explain the process of refund in kind words please.\")\n",
    "print(f\"{result.messages[-1].content[:500]}\\n{'-'*80}\")\n"
   ],
   "id": "3fa5fdf56c5a22a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agent Tool calling",
   "id": "eb668165623a08f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_tax(income: float, tax_rate: float) -> float:\n",
    "    \"\"\"Calculate the tax based on income and tax rate.\"\"\"\n",
    "    return income * tax_rate / 100\n",
    "\n",
    "def mortage_advice(loan_amount: float, interest_rate: float, term_years: int) -> str:\n",
    "    \"\"\"Provide basic mortage advice.\"\"\"\n",
    "    monthly_payment = (loan_amount * (interest_rate / 100) / 12) / (1 - (1 + (interest_rate / 100) / 12) ** (-term_years * 12))\n",
    "    return f\"For a loan amount of {loan_amount} at an interest rate of {interest_rate}% over {term_years} years, your estimated monthly payment is {monthly_payment:.2f}.\"\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"AccountantMorgageBrokerAgent\",\n",
    "    description=\"An expert accountant who can help with tax calculations and financial advice.\",\n",
    "    system_message=\"You are an expert accountant who also is a mortage broker. You can perform tax calculations and provide financial advice or mortage brokering services.\",\n",
    "    tools=[calculate_tax, mortage_advice],\n",
    "    model_client=gemini_client)\n",
    "\n",
    "result = await agent.run(task=\"Calculate the tax for an income of 85000 with a tax rate of 22%.\")\n",
    "print(f\"Your Tax Amount: {result.messages[-1].content[:500]}\\n{'-'*80}\")\n",
    "result = await agent.run(task=\"I want to take a mortage loan of 300000 at an interest rate of 6.5% for a term of 30 years. What will be my monthly payment?\")\n",
    "print(f\"{result.messages[-1].content[:500]}\\n{'-'*80}\")"
   ],
   "id": "53f33bf162daf8ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Messages",
   "id": "62477564c46a4917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "################\n",
    "# Text Message #\n",
    "################\n",
    "agent = AssistantAgent(\n",
    "    name=\"DoctorAgent\",\n",
    "    description=\"GP.\",\n",
    "    system_message=\"You are a a very dismissive general practitioner doctor. You do not entertain any questions that are not related to health.\",\n",
    "    model_client=deepseek_client)\n",
    "textmessage = TextMessage(content=\"I have a 104Â°C fever\", source=\"user\") # Patient mistook Â°F instead of Â°C\n",
    "result = await agent.run(task=textmessage)\n",
    "print(f\"{result.messages[-1].content} \\n{'-'*80}\")\n",
    "\n",
    "#####################################\n",
    "# MultiModal Message (Image + Text) #\n",
    "#####################################\n",
    "agent = AssistantAgent(\n",
    "    name=\"MountainExpertAgent\",\n",
    "    description=\"An expert in mountains and geography.\",\n",
    "    system_message=\"You are an expert in mountains and geography. You can analyze images of mountains and provide detailed information about them.\",\n",
    "    model_client=gemini_client)\n",
    "image = requests.get(\n",
    "    \"https://fastly.picsum.photos/id/866/200/300.jpg?hmac=rcadCENKh4rD6MAp6V_ma-AyWv641M4iiOpe1RyFHeI\",\n",
    "    proxies={\"http\": None, \"https\": None}\n",
    ")\n",
    "ag_image = AGImage(Image.open(BytesIO(image.content)))\n",
    "multimodal_message = MultiModalMessage(\n",
    "    content = [\"In one sentence what is the type of mountain?\", ag_image],\n",
    "    source=\"user\"\n",
    ")\n",
    "result = await agent.run(task=multimodal_message)\n",
    "print(f\"{result.messages[-1].content} \\n{'-'*80}\")"
   ],
   "id": "34cdd0a9671b309e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Running and observing",
   "id": "fbd532e81c02cea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agent = AssistantAgent(\n",
    "    name=\"MarketingAgent\",\n",
    "    description=\"An expert marketing agent.\",\n",
    "    system_message=\"You are an expert marketing agent who is able to sell a marketing product\",\n",
    "    model_client=deepseek_client\n",
    ")\n",
    "result = await agent.on_messages(\n",
    "    messages=[TextMessage(content=\"Marketing agent\", source=\"user\")],\n",
    "    cancellation_token=CancellationToken()\n",
    ")\n",
    "print(result.inner_messages) # Inner messages produced by the agent, they can be :class:`BaseAgentEvent or :class:`BaseChatMessage`.\n",
    "print(result.chat_message) # A chat message produced by the agent as the response."
   ],
   "id": "5bb412f1da9c8379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Streaming with Console UI",
   "id": "e971ba4df731406a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def our_company_marketing_strategy() -> str:\n",
    "    \"\"\"Provides information about our company's marketing targets.\"\"\"\n",
    "    return \"Our company's marketing strategy is to trap customers into buying unnecessary products through lies and aggressive advertising.\"\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"MarketingAgent\",\n",
    "    description=\"An expert marketing agent.\",\n",
    "    system_message=\"You are an expert marketing agent who is able to sell a marketing product\",\n",
    "    model_client=gemini_client,\n",
    "    tools=[our_company_marketing_strategy],\n",
    ")\n",
    "\n",
    "async def progress_callback(output_stats=True) -> None:\n",
    "    await Console(\n",
    "        agent.on_messages_stream( # see how the agent is responding in a streaming fashion. Call Request Event callbacks here.\n",
    "            messages=\n",
    "            [TextMessage(content=\"You are a Marketing agent, your task is to sell raw unprocessed ice to an igloo man. use any tools to find about company specific marketing strategy.\", source=\"user\")],\n",
    "            cancellation_token=CancellationToken()\n",
    "        ),\n",
    "        output_stats = output_stats # Enables stats printing.\n",
    "    )\n",
    "\n",
    "await progress_callback() # Outside of notebook cells, run in an async context\n",
    "print('-'*80)\n",
    "await progress_callback(False)\n"
   ],
   "id": "1c81f1a078db2703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Structured Output with JSON (Need fixing)",
   "id": "b76bd5faa9ca852c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ProductInfo(BaseModel):\n",
    "    product_name: str = Field(..., description=\"Name of the product being marketed.\")\n",
    "    target_audience: str = Field(..., description=\"The target audience for the marketing campaign.\")\n",
    "    key_features: list[str] = Field(..., description=\"List of key features of the product.\")\n",
    "    marketing_channels: list[str] = Field(..., description=\"Recommended marketing channels to reach the target audience.\")\n",
    "\n",
    "structedoutput_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-oss-120b:free\",\n",
    "    api_key=open_router_api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_info={\n",
    "        \"family\": \"gpt-4o\",\n",
    "        \"vision\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False\n",
    "    },\n",
    "    http_client=httpx.AsyncClient(trust_env=False)\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"StructuredOutputMarketingAgent\",\n",
    "    description=\"An expert marketing agent who provides structured output.\",\n",
    "    system_message=(\n",
    "        \"You are an expert marketing agent. \"\n",
    "        \"Respond ONLY in JSON matching this schema: \"\n",
    "        '{\"product_name\": str, \"target_audience\": str, \"key_features\": [str], \"marketing_channels\": [str]}'\n",
    "    ),\n",
    "    model_client=structedoutput_client\n",
    ")\n",
    "print(await agent.run(task=\"respond only json matching the schema with mock values in no more than 200 words.\"))\n",
    "# result = await agent.run(task=\"Provide a marketing strategy for a new eco-friendly water bottle.\")\n",
    "# print(result.messages[-1].content[:500])\n",
    "# structured_output: ProductInfo = result.messages[-1].content"
   ],
   "id": "e9e617787a69a6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multi Agent",
   "id": "5b512a2e57492bc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initiate_marketing_campaign(product_name: str, budget: float) -> str:\n",
    "    \"\"\"Initiates a marketing campaign for a given product within the specified budget.\"\"\"\n",
    "    return f\"Marketing campaign for {product_name} has been initiated with a budget of ${budget:.2f}.\"\n",
    "\n",
    "\n",
    "marketting_head = AssistantAgent(\n",
    "    name=\"MarketingAgent\",\n",
    "    description=\"Marketing Head.\",\n",
    "    system_message=\"You are a expert marketing agent who come up with ideas to sell products effectively.\",\n",
    "    model_client=gemini_client,\n",
    "    tools=[initiate_marketing_campaign]\n",
    ")\n",
    "chief_data_scientist = AssistantAgent(\n",
    "    name=\"DataScientistAgent\",\n",
    "    description=\"Chief Data Scientist.\",\n",
    "        system_message=\"You are able to come up with strategies to Analyse existing Data.\",\n",
    "    model_client=gemini_client\n",
    ")\n",
    "engineering_head = AssistantAgent(\n",
    "    name=\"EngineeringAgent\",\n",
    "    description=\"CTO.\",\n",
    "    system_message=\"You are able to come up with new Ideas and come up with Engineering solutions to it.\",\n",
    "    model_client=gemini_client,\n",
    "    tools=[initiate_marketing_campaign]\n",
    ")\n",
    "# Replace it with Swarm to change from RoundRobin to Swarm.\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=[engineering_head, chief_data_scientist, marketting_head],\n",
    "    max_turns=3\n",
    ")\n",
    "final_message = None\n",
    "# or await team.run and then run loop.\n",
    "async for message in team.run_stream(task=\"Come up with an shipping product idea for shipping related project management\"):\n",
    "    final_message = message\n",
    "    print(f\"{'ðŸš€' * 80}\\n({type(message)}\")\n",
    "    print(f\"type(message) == TaskResult: {type(message) == TaskResult} \\n isinstance(message, TaskResult) {isinstance(message, TaskResult)}\")\n",
    "    if type(message) == TextMessage:\n",
    "        print(f\"{'.' * 80}\\n[{message.source}] {message.content}\")\n",
    "    else: # Task result has no message source or content.\n",
    "        print(message)\n",
    "\n",
    "# Cheeky function inside function\n",
    "async def async_print_stop_reason(message):\n",
    "    print(\"ðŸ”¨\" * 80)\n",
    "    print(message.stop_reason)\n",
    "await async_print_stop_reason(final_message)"
   ],
   "id": "e273738477327710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sum example - surprisingly it doesn't add correctly with Gemini Model only the deepseek model works.\n",
    "message = \"\"\"Increment the received number by exactly 1.\n",
    "Output format: [number only, no text]\n",
    "Mathematical operation: n + 1\"\"\"\n",
    "agent_1 = AssistantAgent(\n",
    "    name=\"Agent1\",\n",
    "    description=\"First agent adds 1. start with 0\",\n",
    "    system_message=message,\n",
    "    model_client=deepseek_client\n",
    ")\n",
    "agent_2 = AssistantAgent(\n",
    "    name=\"Agent2\",\n",
    "    description=\"Second agent adds 1.\",\n",
    "    system_message=message,\n",
    "    model_client=deepseek_client\n",
    ")\n",
    "\n",
    "agent_3 = AssistantAgent(\n",
    "    name=\"Agent3\",\n",
    "    description=\"Third agent adds 1.\",\n",
    "    system_message=message,\n",
    "    model_client=deepseek_client\n",
    ")\n",
    "# max_turns is the stop condition here\n",
    "team = RoundRobinGroupChat(\n",
    "    [agent_1, agent_2, agent_3],\n",
    "    max_turns=3\n",
    ")\n",
    "\n",
    "\n",
    "await Console(team.run(task=\"Start Counting....\"))\n",
    "print(\"-\"*80)\n",
    "await Console(team.run())\n"
   ],
   "id": "daadd16a48dc6b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d1f25f8be7e98a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
